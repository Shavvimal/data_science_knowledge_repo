{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CSHdHVDP0t96"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p0qs8Lh10t99"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Cqg2gAxkzOF2"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_processed_plant_item_encoded_13_03_22.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(series):\n",
    "    \"\"\"Fills missing values. \n",
    "        Interpolate missing values with a linear approximation.\n",
    "    \"\"\"\n",
    "    series_filled = series.interpolate(method='linear')\n",
    "    return series_filled\n",
    "        \n",
    "    \n",
    "def scale(X):\n",
    "    mm = MinMaxScaler()\n",
    "    X_ = np.atleast_2d(X)\n",
    "    return pd.DataFrame(mm.fit(X_[:23,:]).transform(X_), X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "for col in ['PlantID', 'ParentItemID']:\n",
    "    data[col] = data[col].astype(str).astype(\"category\")\n",
    "data['Volume'] = data['Volume'].astype(float)\n",
    "data.drop(['month'], axis=1, inplace=True)\n",
    "data.drop(['index.1'], axis=1, inplace=True)\n",
    "data[\"month\"] = data.Date.dt.month.astype(str).astype(\"category\")  # categories have be strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shav.vimelindiran\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\shav.vimelindiran\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\shav.vimelindiran\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\shav.vimelindiran\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# Split the Validation and Train dataset by time_index\n",
    "validations = data[data['time_idx'].isin(list(range(6, 35)))]\n",
    "train = data[data['time_idx'].isin(list(range(0, 29)))] # generates 0 to 28\n",
    "\n",
    "# These are the columns to minmax scale. Currently, they are ['Volume', 'log_volume', 'log_ret', 'avg_volume_by_material',\n",
    "      #  'max_volume_by_material', 'min_volume_by_material',\n",
    "      #  'std_volume_by_material', 'avg_volume_by_plant', 'max_volume_by_plant',\n",
    "      #  'min_volume_by_plant', 'std_volume_by_plant', 'encoding_1',\n",
    "      #  'encoding_2', 'encoding_3']\n",
    "\n",
    "cols = validations.columns[[3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19]]\n",
    "\n",
    "validations[cols + '_scaled'] = validations.groupby('timeseries')[cols].apply(scale)\n",
    "validations.drop(cols, axis=1, inplace=True)\n",
    "validations.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "train[cols + '_scaled'] = train.groupby('timeseries')[cols].apply(scale)\n",
    "train.drop(cols, axis=1, inplace=True)\n",
    "train.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "# One Hot Encode everyting that is categorical or Object # train.dtypes\n",
    "train = pd.get_dummies(train)\n",
    "validations = pd.get_dummies(validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivarate Datasets\n",
      "Train Data Shape: (303079, 1123)\n",
      "Val Data Shape: (303079, 1123)\n",
      "Nulls In Train False\n",
      "Nulls In Validation False\n"
     ]
    }
   ],
   "source": [
    "print(\"Multivarate Datasets\")\n",
    "print(f\"Train Data Shape: {train.shape}\")\n",
    "print(f\"Val Data Shape: {validations.shape}\")\n",
    "print(f\"Nulls In Train {np.any(np.isnan(train))}\")\n",
    "print(f\"Nulls In Validation {np.any(np.isnan(validations))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 3d matrices \n",
    "validations_matr = np.array(list(validations.groupby('timeseries').apply(pd.DataFrame.to_numpy)))[:,:,3:]\n",
    "train_matr = np.array(list(train.groupby('timeseries').apply(pd.DataFrame.to_numpy)))[:,:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('validations_matr.npy', validations_matr)\n",
    "np.save('train_matr.npy', train_matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "validations_matr = np.load('validations_matr.npy')\n",
    "train_matr = np.load('train_matr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10451, 29, 1120)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations_matr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42937853, 1.        , 0.74011299, 0.59322034, 0.        ,\n",
       "       0.00564972, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.84745763, 0.30508475, 0.45762712, 0.63841808, 0.11864407,\n",
       "       0.30225989, 0.04519774, 0.01694915, 0.01129944, 0.        ,\n",
       "       0.        , 0.56497175, 0.92655367, 0.57344633, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations_matr[0, : , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(validations_matr)\n",
    "# ds = ds.batch(32).prefetch(1)\n",
    "# print(f'Num. of Batchs: {len(ds)} in {ds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.window(29, shift=6, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.flat_map(lambda x : x.batch(29))\n",
    "ds = ds.shuffle(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (29, 1120), types: tf.float64>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000002933C03F790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x000002933C03F790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(lambda x : (x[:23, :], x[-6:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((23, 1120), (6,)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of Batchs: 10451 in <MapDataset shapes: ((23, 1120), (6,)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "print(f'Num. of Batchs: {len(ds)} in {ds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of Batchs: 327 in <BatchDataset shapes: ((None, 23, 1120), (None, 6)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "ds = ds.batch(32)\n",
    "print(f'Num. of Batchs: {len(ds)} in {ds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of Batchs: 327 in <BatchDataset shapes: ((None, 23, 1120), (None, 6)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "print(f'Num. of Batchs: {len(ds)} in {ds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dataset length is unknown.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SHAV~1.VIM\\AppData\\Local\\Temp/ipykernel_19396/3397119741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Num. of Batchs: {len(ds)} in {ds}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    445\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset length is infinite.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mUNKNOWN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset length is unknown.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dataset length is unknown."
     ]
    }
   ],
   "source": [
    "print(f'Num. of Batchs: {len(ds)} in {ds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.data' has no attribute 'validations_matr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SHAV~1.VIM\\AppData\\Local\\Temp/ipykernel_19396/3783624381.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidations_matr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.data' has no attribute 'validations_matr'"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RangeDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(ds, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93184ff88d627996db6ae820626d3e55d27fde3f04aa3091aab9dfcbc84df5cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
