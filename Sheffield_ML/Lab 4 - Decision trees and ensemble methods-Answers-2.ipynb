{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees and ensemble methods\n",
    "\n",
    "*Solutions to the questions were prepared by Mr Chunchao Ma (PhD candidate)*\n",
    "\n",
    "A [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) can be thought of as a sequence of **hierarchical if-else statements** that test feature values to predict a class.\n",
    "\n",
    "\n",
    "In this notebook we will explore the use of [scikit-learn](https://scikit-learn.org/stable/) for Decision Trees. This first example will allow us to understand some of the parameters in a decision tree. \n",
    "\n",
    "\n",
    "## Decision trees with scikit-learn\n",
    "\n",
    "We will build a classifier that will be able to detect spam from the text in an email. The dataset that we will use is from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php), where UCI stands for University of California Irvine. The UCI repository is and has been a valuable resource in Machine Learning. It contains datasets for classification, regression, clustering and several other machine learning problems. These datasets are open source and they have been uploaded by contributors of many research articles. \n",
    "\n",
    "The particular dataset that we will use wil be referred to is the [Spambase Dataset](http://archive.ics.uci.edu/ml/datasets/Spambase). A detailed description is in the previous link. The dataset contains 57 features related to word frequency, character frequency, and others related to capital letters. The description of the features and labels in the dataset is available [here](http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names). The output label indicated whether an email was considered 'ham' or 'spam', so it is a binary label. \n",
    "\n",
    "We will use Decision tree as our predictive model. But first, we get the data and the names of the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('./datasets/spambase.data', header=None)\n",
    "spam_names = [spam_names.rstrip('\\n') for spam_names in open('./datasets/spambase.data.names')]\n",
    "number_names = np.shape(spam_names)[0]\n",
    "for i in range(number_names):\n",
    "    local = spam_names[i]\n",
    "    colon_pos = local.find(':')\n",
    "    spam_names[i] = local[:colon_pos]\n",
    "spam_data.columns = spam_names\n",
    "X = spam_data.iloc[:, 0:57]\n",
    "y = spam_data.iloc[:, 57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam_ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total     spam_ham  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the whole dataset to build a decision tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** to visualize the tree, we can use the [Graphviz](http://www.graphviz.org/) package and use the exporter [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz). First, we need to install the package. An easy way to do it is to use `conda`\n",
    "\n",
    "`conda install python-graphviz`\n",
    "\n",
    "If you are running the Notebook in Binder, you do not need to worry about installing graphviz.\n",
    "\n",
    "We will export the tree as a pdf file spam.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam.pdf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz) to customize several aspects of the tree. For example, if you look at the .pdf file generated, the names of the features are assigned by default by refering to the column index in `X`. It is possible to assign the names of the features directly. Likewise for the labels `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=spam_names[0:57],  \n",
    "                      class_names=spam_names[57],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of printing a new .pdf file, we can render the graph inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"1039pt\" height=\"433pt\"\n",
       " viewBox=\"0.00 0.00 1039.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-429 1035,-429 1035,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#f6d3ba\" stroke=\"black\" d=\"M588,-425C588,-425 472,-425 472,-425 466,-425 460,-419 460,-413 460,-413 460,-354 460,-354 460,-348 466,-342 472,-342 472,-342 588,-342 588,-342 594,-342 600,-348 600,-354 600,-354 600,-413 600,-413 600,-419 594,-425 588,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"469\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">char_freq_$ ≤ 0.056</text>\n",
       "<text text-anchor=\"start\" x=\"482.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.967</text>\n",
       "<text text-anchor=\"start\" x=\"481.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4601</text>\n",
       "<text text-anchor=\"start\" x=\"468\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2788, 1813]</text>\n",
       "<text text-anchor=\"start\" x=\"503\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#eda876\" stroke=\"black\" d=\"M519.5,-306C519.5,-306 364.5,-306 364.5,-306 358.5,-306 352.5,-300 352.5,-294 352.5,-294 352.5,-235 352.5,-235 352.5,-229 358.5,-223 364.5,-223 364.5,-223 519.5,-223 519.5,-223 525.5,-223 531.5,-229 531.5,-235 531.5,-235 531.5,-294 531.5,-294 531.5,-300 525.5,-306 519.5,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"360.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_remove ≤ 0.055</text>\n",
       "<text text-anchor=\"start\" x=\"394.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.787</text>\n",
       "<text text-anchor=\"start\" x=\"393.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3471</text>\n",
       "<text text-anchor=\"start\" x=\"384\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2655, 816]</text>\n",
       "<text text-anchor=\"start\" x=\"415\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M499.47,-341.91C492.71,-332.92 485.48,-323.32 478.52,-314.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"481.29,-311.91 472.48,-306.02 475.69,-316.12 481.29,-311.91\"/>\n",
       "<text text-anchor=\"middle\" x=\"468.98\" y=\"-327.07\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#53aae8\" stroke=\"black\" d=\"M698,-306C698,-306 588,-306 588,-306 582,-306 576,-300 576,-294 576,-294 576,-235 576,-235 576,-229 582,-223 588,-223 588,-223 698,-223 698,-223 704,-223 710,-229 710,-235 710,-235 710,-294 710,-294 710,-300 704,-306 698,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"584\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_hp ≤ 0.4</text>\n",
       "<text text-anchor=\"start\" x=\"595.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.523</text>\n",
       "<text text-anchor=\"start\" x=\"594.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1130</text>\n",
       "<text text-anchor=\"start\" x=\"588.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [133, 997]</text>\n",
       "<text text-anchor=\"start\" x=\"615.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M569.2,-341.91C578.06,-332.74 587.54,-322.93 596.65,-313.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"599.43,-315.65 603.86,-306.02 594.4,-310.78 599.43,-315.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"604.3\" y=\"-327.32\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ea9a60\" stroke=\"black\" d=\"M268,-187C268,-187 158,-187 158,-187 152,-187 146,-181 146,-175 146,-175 146,-116 146,-116 146,-110 152,-104 158,-104 158,-104 268,-104 268,-104 274,-104 280,-110 280,-116 280,-116 280,-175 280,-175 280,-181 274,-187 268,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"154\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">char_freq_! ≤ 0.191</text>\n",
       "<text text-anchor=\"start\" x=\"165.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.644</text>\n",
       "<text text-anchor=\"start\" x=\"164.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3141</text>\n",
       "<text text-anchor=\"start\" x=\"155\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2625, 516]</text>\n",
       "<text text-anchor=\"start\" x=\"186\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M362.55,-222.91C338.72,-210.73 312.66,-197.42 289.06,-185.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.64,-182.24 280.14,-180.8 287.45,-188.47 290.64,-182.24\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#4da7e8\" stroke=\"black\" d=\"M514,-187C514,-187 370,-187 370,-187 364,-187 358,-181 358,-175 358,-175 358,-116 358,-116 358,-110 364,-104 370,-104 370,-104 514,-104 514,-104 520,-104 526,-110 526,-116 526,-116 526,-175 526,-175 526,-181 520,-187 514,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"366\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_george ≤ 0.14</text>\n",
       "<text text-anchor=\"start\" x=\"394.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.439</text>\n",
       "<text text-anchor=\"start\" x=\"397\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 330</text>\n",
       "<text text-anchor=\"start\" x=\"391.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [30, 300]</text>\n",
       "<text text-anchor=\"start\" x=\"414.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M442,-222.91C442,-214.65 442,-205.86 442,-197.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"445.5,-197.02 442,-187.02 438.5,-197.02 445.5,-197.02\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#e78c4b\" stroke=\"black\" d=\"M120,-68C120,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 120,0 120,0 126,0 132,-6 132,-12 132,-12 132,-56 132,-56 132,-62 126,-68 120,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"18.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.412</text>\n",
       "<text text-anchor=\"start\" x=\"17.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2524</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2315, 209]</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.26,-103.73C145.41,-94.15 131.73,-83.96 118.95,-74.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.81,-71.47 110.7,-68.3 116.63,-77.08 120.81,-71.47\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#fffefd\" stroke=\"black\" d=\"M263.5,-68C263.5,-68 162.5,-68 162.5,-68 156.5,-68 150.5,-62 150.5,-56 150.5,-56 150.5,-12 150.5,-12 150.5,-6 156.5,0 162.5,0 162.5,0 263.5,0 263.5,0 269.5,0 275.5,-6 275.5,-12 275.5,-12 275.5,-56 275.5,-56 275.5,-62 269.5,-68 263.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"173\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n",
       "<text text-anchor=\"start\" x=\"168\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 617</text>\n",
       "<text text-anchor=\"start\" x=\"158.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [310, 307]</text>\n",
       "<text text-anchor=\"start\" x=\"186\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213,-103.73C213,-95.52 213,-86.86 213,-78.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"216.5,-78.3 213,-68.3 209.5,-78.3 216.5,-78.3\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#44a3e6\" stroke=\"black\" d=\"M398.5,-68C398.5,-68 305.5,-68 305.5,-68 299.5,-68 293.5,-62 293.5,-56 293.5,-56 293.5,-12 293.5,-12 293.5,-6 299.5,0 305.5,0 305.5,0 398.5,0 398.5,0 404.5,0 410.5,-6 410.5,-12 410.5,-12 410.5,-56 410.5,-56 410.5,-62 404.5,-68 398.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"304.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.302</text>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 317</text>\n",
       "<text text-anchor=\"start\" x=\"301.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [17, 300]</text>\n",
       "<text text-anchor=\"start\" x=\"324.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M408.49,-103.73C401.07,-94.7 393.2,-85.12 385.77,-76.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"388.42,-73.8 379.37,-68.3 383.02,-78.25 388.42,-73.8\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M519,-68C519,-68 441,-68 441,-68 435,-68 429,-62 429,-56 429,-56 429,-12 429,-12 429,-6 435,0 441,0 441,0 519,0 519,0 525,0 531,-6 531,-12 531,-12 531,-56 531,-56 531,-62 525,-68 519,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"440\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"439\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13</text>\n",
       "<text text-anchor=\"start\" x=\"437\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [13, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"453\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M456.15,-103.73C459.06,-95.34 462.14,-86.47 465.08,-78.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"468.47,-78.89 468.44,-68.3 461.86,-76.6 468.47,-78.89\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#47a4e7\" stroke=\"black\" d=\"M705.5,-187C705.5,-187 580.5,-187 580.5,-187 574.5,-187 568.5,-181 568.5,-175 568.5,-175 568.5,-116 568.5,-116 568.5,-110 574.5,-104 580.5,-104 580.5,-104 705.5,-104 705.5,-104 711.5,-104 717.5,-110 717.5,-116 717.5,-116 717.5,-175 717.5,-175 717.5,-181 711.5,-187 705.5,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"576.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_edu ≤ 0.49</text>\n",
       "<text text-anchor=\"start\" x=\"595.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.351</text>\n",
       "<text text-anchor=\"start\" x=\"594.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1060</text>\n",
       "<text text-anchor=\"start\" x=\"592.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [70, 990]</text>\n",
       "<text text-anchor=\"start\" x=\"615.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M643,-222.91C643,-214.65 643,-205.86 643,-197.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"646.5,-197.02 643,-187.02 639.5,-197.02 646.5,-197.02\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#e88f4f\" stroke=\"black\" d=\"M938.5,-187C938.5,-187 783.5,-187 783.5,-187 777.5,-187 771.5,-181 771.5,-175 771.5,-175 771.5,-116 771.5,-116 771.5,-110 777.5,-104 783.5,-104 783.5,-104 938.5,-104 938.5,-104 944.5,-104 950.5,-110 950.5,-116 950.5,-116 950.5,-175 950.5,-175 950.5,-181 944.5,-187 938.5,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"779.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_remove ≤ 0.075</text>\n",
       "<text text-anchor=\"start\" x=\"813.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.469</text>\n",
       "<text text-anchor=\"start\" x=\"820\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 70</text>\n",
       "<text text-anchor=\"start\" x=\"818\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [63, 7]</text>\n",
       "<text text-anchor=\"start\" x=\"834\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M710.19,-227.44C731.08,-216.23 754.38,-203.72 776.31,-191.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"778.13,-194.95 785.28,-187.14 774.82,-188.78 778.13,-194.95\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#44a2e6\" stroke=\"black\" d=\"M654.5,-68C654.5,-68 561.5,-68 561.5,-68 555.5,-68 549.5,-62 549.5,-56 549.5,-56 549.5,-12 549.5,-12 549.5,-6 555.5,0 561.5,0 561.5,0 654.5,0 654.5,0 660.5,0 666.5,-6 666.5,-12 666.5,-12 666.5,-56 666.5,-56 666.5,-62 660.5,-68 654.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"560.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.297</text>\n",
       "<text text-anchor=\"start\" x=\"559.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1045</text>\n",
       "<text text-anchor=\"start\" x=\"557.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [55, 990]</text>\n",
       "<text text-anchor=\"start\" x=\"580.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M629.97,-103.73C627.29,-95.34 624.45,-86.47 621.75,-78.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"625.02,-76.76 618.64,-68.3 618.35,-78.89 625.02,-76.76\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M775,-68C775,-68 697,-68 697,-68 691,-68 685,-62 685,-56 685,-56 685,-12 685,-12 685,-6 691,0 697,0 697,0 775,0 775,0 781,0 787,-6 787,-12 787,-12 787,-56 787,-56 787,-62 781,-68 775,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"696\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"695\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\n",
       "<text text-anchor=\"start\" x=\"693\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [15, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"709\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M677.63,-103.73C685.3,-94.7 693.43,-85.12 701.11,-76.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"703.91,-78.19 707.72,-68.3 698.58,-73.66 703.91,-78.19\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#e5833c\" stroke=\"black\" d=\"M904.5,-68C904.5,-68 817.5,-68 817.5,-68 811.5,-68 805.5,-62 805.5,-56 805.5,-56 805.5,-12 805.5,-12 805.5,-6 811.5,0 817.5,0 817.5,0 904.5,0 904.5,0 910.5,0 916.5,-6 916.5,-12 916.5,-12 916.5,-56 916.5,-56 916.5,-62 910.5,-68 904.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"813.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.116</text>\n",
       "<text text-anchor=\"start\" x=\"820\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 64</text>\n",
       "<text text-anchor=\"start\" x=\"818\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [63, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"834\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M861,-103.73C861,-95.52 861,-86.86 861,-78.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"864.5,-78.3 861,-68.3 857.5,-78.3 864.5,-78.3\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M1019,-68C1019,-68 947,-68 947,-68 941,-68 935,-62 935,-56 935,-56 935,-12 935,-12 935,-6 941,0 947,0 947,0 1019,0 1019,0 1025,0 1031,-6 1031,-12 1031,-12 1031,-56 1031,-56 1031,-62 1025,-68 1019,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"943\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"945.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"943.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 6]</text>\n",
       "<text text-anchor=\"start\" x=\"955.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M906.43,-103.73C916.9,-94.33 928.02,-84.35 938.45,-74.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"940.79,-77.58 945.9,-68.3 936.12,-72.37 940.79,-77.58\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fe423d9b6d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could quickly use the tree generated for the purposes of [feature selection](https://en.wikipedia.org/wiki/Feature_selection). Feature selection is a whole research area in machine learning with a very practical purpose: being able to identify which features are more relevant in a prediction problem. In modern big data applications, the amount of features generated is huge. For example, one could extract thousands of millions of features from a genome sequence that maps to a particular medical disorder. Finding which features are more relevant for correctly classifying the disorder could lead to breakthroughs in medicine. \n",
    "\n",
    "Relevant features can be identified starting from the top level of the tree and going down to the leaf nodes. For example, one can argue that the most important feature is the one used in the root node (e.g. char_freq_$ for the Spambase dataset) since it has the highest entropy.    \n",
    "\n",
    "## Evaluating the prediction ability of a decision tree classifier\n",
    "\n",
    "We will now evaluate the predictive ability of the decision tree classifier on a subset of the Spam dataset. The Decision Tree has several tunable parameters, including, the criterion or impurity measure (criterion) and the maximum depth of the tree (max_depth). A complete list of parameters for the DecisionTreeClassifier implemented in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).\n",
    "\n",
    "We will create a simple Grid Search for crosvalidating the best parameters for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first split the data into a train and a test set. \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "indexes = list(ss.split(X, y))\n",
    "train_set  = indexes[0][0]\n",
    "test_set  = indexes[0][1]\n",
    "Xtrain = X.iloc[train_set, :]\n",
    "ytrain = y.iloc[train_set]\n",
    "Xtest = X.iloc[test_set, :]\n",
    "ytest = y.iloc[test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a Grid search for the parameters criterion and max_depth and we use the training data to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=42, test_size=0.3, train_size=None),\n",
       "             estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': array(['entropy', 'gini'], dtype='<U7'),\n",
       "                         'max_depth': [3, 5, 10, 15]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV # For model selection\n",
    "criterion_opts = np.array(['entropy', 'gini'])\n",
    "max_depth_opts = [3, 5, 10, 15]\n",
    "param_grid = dict(criterion = criterion_opts, max_depth = max_depth_opts)\n",
    "cv = ShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "grid = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "grid.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now which ones were the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train a new decision tree using those parameters and then evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=grid.best_params_[\"criterion\"],max_depth=grid.best_params_[\"max_depth\"])\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9094858797972484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # Accuracy score\n",
    "accuracy = accuracy_score(ytest, ypred)\n",
    "print(accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the labels\n",
    "\n",
    "Let us look at the distribution of the instances for class in the original dataset and in the training data. \n",
    "\n",
    ">**Warning for MAC OS users** As of today, the current installation of `graphviz` **may** put its own version of freetype into the default python runtime library path. However, matplotlib needs a different version of the same library. The fix is explained in [this entry of stackoverflow](https://stackoverflow.com/questions/28028786/matplotlib-error-libfreetype-6-dylib). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "y.hist()\n",
    "plt.ylabel('Number of instances')\n",
    "plt.xlabel('Class label')\n",
    "plt.title('Class distribution for X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ytrain.hist()\n",
    "plt.ylabel('Number of instances')\n",
    "plt.xlabel('Class label')\n",
    "plt.title('Class distribution for Xtrain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the numbers of data observations per class are different. We usually refer to this kind of problems as \"imbalanced\". Bear in mind that when creating your training data, you need to account for this imbalance and be aware against misleading results. For example, accuracy is probably not a good metric to measure performance in these problems. Say you have a binary classification problem for which your test data consists of 100 instances. From those, 99 instances belong to class 1, say 'ham', and 1 instance belong to class 2, say 'spam'. When you test your classifier on these instances, you could have a classifier that only is able to predict class 1 correctly, but elements of class 2 are predicted incorrectly. If you apply this classifier to your test data, you would get 99% accuracy. Looking at this percentage out of context and saying that you have produced a highly realiable classifier is completely wrong since your classifier only learned to correctly classify instances in class 1. The problem gets worse if your test set is large, say 100,000 instances, 99,000 are correctly predicted for class 1 and 1,000 are incorrectly predicted for class 2. Even when the classifier fails in correctly predicted 1,000 instances from class 2, the accuracy of this classifier would be 99%.\n",
    "\n",
    "There are different [strategies for balancing a dataset](https://books.google.co.uk/books/about/Imbalanced_Learning.html?id=YqQJngEACAAJ&redir_esc=y) but if for some reason you are not able to balance it, it is important to be aware of these issues. Some stategies include: i) under-sampling the majority class(es), ii) over-sampling the minority class, and iii) combining over- and under-sampling. The python package [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn#id22) has several implementations of these different strategies. You can install the package and try some of them.\n",
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "A simple strategy we can use to balance the problem for the Spam dataset is to under-sample the majority class. Repeat the spam prediction problem above but make sure you have the same number of samples in both classes. \n",
    "Do you notice any difference in the accuracy over the same test set that we used before? Also, use a performance measure that takes into account imbalanced data. Do you see any difference between this new performance measure before and after balancing the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9015206372194062\n",
      "The performance for model in question in resampled dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       804\n",
      "           1       0.86      0.91      0.89       577\n",
      "\n",
      "    accuracy                           0.90      1381\n",
      "   macro avg       0.90      0.90      0.90      1381\n",
      "weighted avg       0.90      0.90      0.90      1381\n",
      "\n",
      "The performance for the model in original dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92       804\n",
      "           1       0.91      0.87      0.89       577\n",
      "\n",
      "    accuracy                           0.91      1381\n",
      "   macro avg       0.91      0.90      0.91      1381\n",
      "weighted avg       0.91      0.91      0.91      1381\n",
      "\n",
      "The recall for question 1 model is:\n",
      "0.9098786828422877\n",
      "The recall for original model is:\n",
      "0.8665511265164645\n"
     ]
    }
   ],
   "source": [
    "# Provide your answer here\n",
    "\n",
    "####### After installing the python package imbalanced-learn,\n",
    "####### we follow one of the under-sample in imbalanced-learn.\n",
    "####### For more detail: https://imbalanced-learn.org/stable/under_sampling.html\n",
    "\n",
    "## Implement one of under-sample method in imbalanced-learn\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=0)\n",
    "X_resampled, y_resampled = cc.fit_resample(Xtrain, ytrain)\n",
    "\n",
    "## recheck whether X_resampled, y_resampled are a balanced dataset\n",
    "y_resampled.hist()\n",
    "plt.ylabel('Number of instances')\n",
    "plt.xlabel('Class label')\n",
    "plt.title('Class distribution for X_resampled')\n",
    "plt.show()\n",
    "## The resampled dataset has same number of samples in both classses.\n",
    "\n",
    "## Now we repreat the spam prediction problem above\n",
    "# choose parameter\n",
    "criterion_opts_question1 = np.array(['entropy', 'gini'])\n",
    "max_depth_opts_question1 = [3, 5, 10, 15]\n",
    "param_grid_question1 = dict(criterion = criterion_opts_question1, max_depth = max_depth_opts_question1)\n",
    "cv_question1 = ShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "grid_question1 = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid_question1, cv=cv_question1, scoring='accuracy')\n",
    "grid_question1.fit(X_resampled, y_resampled)\n",
    "\n",
    "# fit model and make prediction\n",
    "clf_question1 = tree.DecisionTreeClassifier(criterion=grid_question1.best_params_[\"criterion\"],max_depth=grid_question1.best_params_[\"max_depth\"])\n",
    "clf_question1.fit(X_resampled, y_resampled)\n",
    "ypred_question1 = clf_question1.predict(Xtest)    \n",
    "\n",
    "# check the accuracy\n",
    "accuracy_question1 = accuracy_score(ytest, ypred_question1)\n",
    "print('The accuracy is',accuracy_question1) \n",
    "\n",
    "# The accuracy over the same test set is quite similar.\n",
    "# If we try run different times, the result can either be slightly better or worse than the original accuracy.\n",
    "\n",
    "## Using a performance measure:\n",
    "## different performance measure: precision, recall, f1-score\n",
    "from sklearn.metrics import classification_report\n",
    "print('The performance for model in question in resampled dataset:')\n",
    "print(classification_report(ytest, ypred_question1))\n",
    "print('The performance for the model in original dataset:')\n",
    "print(classification_report(ytest, ypred))\n",
    "\n",
    "## recall performance measure\n",
    "from sklearn.metrics import recall_score\n",
    "print('The recall for question 1 model is:')\n",
    "print(recall_score(ytest, ypred_question1))\n",
    "print('The recall for original model is:')\n",
    "print(recall_score(ytest, ypred))\n",
    "\n",
    "\n",
    "## The result are similar between performance measure before and after balancing the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees for regression\n",
    "\n",
    "The main difference between Decision Tress for Classification and Decision Trees for Regression is in the impurity measure used. The [decision trees for regression implemented in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) uses the mean squared error by default as the impurity measure. The mean squared error is closely related with the variance, the impurity measure we introduced in the Session for this week. \n",
    "\n",
    "We are going to go back to the dataset of Bike rentals that we used in Lab 2 and compare the performance of the decision tree for regression over the same partitions of train, validation and test sets that we used back there for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv', './SeoulBikeData.csv')\n",
    "bike_sharing_data = pd.read_csv('SeoulBikeData.csv', encoding= 'unicode_escape')\n",
    "bike_sharing_data = bike_sharing_data.drop('Date', axis=1)\n",
    "# We transform the int64 variables in the dataset to float64.\n",
    "for col in ['Rented Bike Count', 'Hour', 'Humidity(%)', 'Visibility (10m)']:\n",
    "    bike_sharing_data[col] = bike_sharing_data[col].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test sets. Have a look at the random_state. We use the same number from Lab 2 to make sure the splits into train and test are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "bs_train_set, bs_test_set = train_test_split(bike_sharing_data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the same data preprocessing step for the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "attributes_cat = ['Seasons', 'Holiday', 'Functioning Day']\n",
    "attributes_num = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', \\\n",
    "                  'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']   \n",
    "\n",
    "full_transform = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), attributes_num),\n",
    "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                 ['Hour', 'Temperature(°C)', 'Humidity(%)',\n",
       "                                  'Wind speed (m/s)', 'Visibility (10m)',\n",
       "                                  'Dew point temperature(°C)',\n",
       "                                  'Solar Radiation (MJ/m2)', 'Rainfall(mm)',\n",
       "                                  'Snowfall (cm)']),\n",
       "                                ('cat', OneHotEncoder(),\n",
       "                                 ['Seasons', 'Holiday', 'Functioning Day'])])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further create a train and a validation set from the original train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train2_set, bs_val_set = train_test_split(bs_train_set, test_size=0.15, random_state=42)\n",
    "bs_train2_set_attributes = bs_train2_set.drop('Rented Bike Count', axis=1)\n",
    "bs_train2_set_labels = bs_train2_set['Rented Bike Count']\n",
    "bs_val_set_attributes = bs_val_set.drop('Rented Bike Count', axis=1)\n",
    "bs_val_set_labels = bs_val_set['Rented Bike Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit transform the attributes in the train set and transform them in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit transform in the train set\n",
    "bs_train2_set_attributes_transformed = full_transform.fit_transform(bs_train2_set_attributes)\n",
    "# transform in the validation set\n",
    "bs_val_set_attributes_transformed = full_transform.transform(bs_val_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to perform a GridSearchCV on the same validation data that we used for Lab 2, we will use [`PredefinedSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit) to tell the cross validator which instances to use for training and which ones for validation. We create first a test_fold array of the same dimensionality than the original training data and assign the value of -1 to the indexes corresponding to train instances and 0 to the indexes corresponding to validation instances. We will then stack the input attributes for both sets and also stack the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "test_fold = np.zeros((np.shape(bs_train_set)[0], 1))\n",
    "test_fold[0:np.shape(bs_train2_set)[0]] = -1\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the attributes and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_train_set_attributes = np.vstack((bs_train2_set_attributes_transformed , bs_val_set_attributes_transformed))\n",
    "whole_train_set_labels = np.hstack((bs_train2_set_labels, bs_val_set_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the decision tree for regression and explore different maximum depth options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             estimator=DecisionTreeRegressor(),\n",
       "             param_grid={'max_depth': [3, 5, 10, 15]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth_opts = [3, 5, 10, 15]\n",
    "param_grid = dict(max_depth = max_depth_opts)\n",
    "grid_regression = GridSearchCV(tree.DecisionTreeRegressor(), param_grid=param_grid, cv=ps, scoring='neg_mean_squared_error')\n",
    "grid_regression.fit(whole_train_set_attributes, whole_train_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train now a decision tree regressor using the best value for the max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = tree.DecisionTreeRegressor(max_depth=grid_regression.best_params_[\"max_depth\"])\n",
    "regr.fit(bs_train2_set_attributes_transformed, bs_train2_set_labels)\n",
    "bs_val_set_predictions = regr.predict(bs_val_set_attributes_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we compute the RMSE for the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288.0807739647666"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "error_mod = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions))\n",
    "error_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice how this is a great improvement compared to the result obtained using linear regression in Lab Notebook 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Decision trees do not require any scaling of the features. Use the same splits of the data than before but use the numerical features as they come, this is, do not use StandardScaler() for the numerical features. What is the RMSE on the validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the validation data is : 281.0584033688919\n"
     ]
    }
   ],
   "source": [
    "# Provide your answer here\n",
    "\n",
    "### We do not use StandardScaler() for the numerical features and keep it as original values\n",
    "Question2_transform = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
    "],remainder='passthrough')\n",
    "\n",
    "\n",
    "## fit transform in the train set\n",
    "bs_train2_set_attributes_Question2 = Question2_transform.fit_transform(bs_train2_set_attributes)\n",
    "## transform in the validation set\n",
    "bs_val_set_attributes_Question2 = Question2_transform.transform(bs_val_set_attributes)\n",
    "\n",
    "## Concatenating the attributes and the labels\n",
    "whole_train_set_attributes_Question2 = np.vstack((bs_train2_set_attributes_Question2 , bs_val_set_attributes_Question2))\n",
    "whole_train_set_labels_Question2 = np.hstack((bs_train2_set_labels, bs_val_set_labels))\n",
    "\n",
    "\n",
    "## Applying the decision tree for regression and exploring different maximum depth options\n",
    "max_depth_opts_Question2 = [3, 5, 10, 15]\n",
    "param_grid_Question2 = dict(max_depth = max_depth_opts_Question2)\n",
    "grid_regression_Question2 = GridSearchCV(tree.DecisionTreeRegressor(), param_grid=param_grid_Question2, cv=ps, scoring='neg_mean_squared_error')\n",
    "grid_regression_Question2.fit(whole_train_set_attributes_Question2, whole_train_set_labels_Question2)\n",
    "\n",
    "## Training a decision tree regressor using the best value for the max depth\n",
    "regr_Question2 = tree.DecisionTreeRegressor(max_depth=grid_regression_Question2.best_params_[\"max_depth\"])\n",
    "regr_Question2.fit(bs_train2_set_attributes_Question2, bs_train2_set_labels)\n",
    "bs_val_set_predictions_Question2 = regr_Question2.predict(bs_val_set_attributes_Question2)\n",
    "\n",
    "## Computing the RMSE for the validation dataset\n",
    "error_mod_Question2 = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions_Question2))\n",
    "print('The RMSE on the validation data is :',error_mod_Question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "In machine learning, we use the term ensemble model to refer to a predictive model that is a composition of several other predictive models. For example, for a classification problem, we can have an ensemble of three classifiers, where the first of them is a decision tree classifier, the second one is a logistic regressor (to be studied in Session 6) and the third one is a shallow neural network. We can train all classifiers with the same training data and then, at test time, predictions can be done using majority voting. \n",
    "\n",
    "Ensemble methods are very popular since they usually show higher performance when compared to simpler classifiers. In fact, gradient boosting trees are the most popular method in [**Kaggle**](https://www.kaggle.com/), a platform that hosts data science competitions. The top entry in the [**Netflix Prize**](https://en.wikipedia.org/wiki/Netflix_Prize) Competition, one of the most famous data science competitions, was based on an ensemble predictive model. \n",
    "\n",
    "The most commmon ensemble methods use decision trees as the members of the ensemble. Scikit-learn implemenst two types of Tree Ensembles, random forests and gradient boosting. The main difference between both methods is the way in which they combine the different trees that compose the ensemble.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "The tree ensemble in random forests is built by training individual decision trees on different subsets of the training data and using a subset of the available features. For classification, the prediction is done by majority voting among the individual trees. In fact, according to Scikit-learn documentation for the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) \"The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.\" For regression, the prediction is the average of the individual predictions of each tree. \n",
    "\n",
    "Some of the additional parameters required in the Random Forest implementation in Scikit-learn include\n",
    "\n",
    "> **n_estimators** the total number of trees to train<p>\n",
    "**max_features** number of features to use as candidates for splitting at each tree node. <p>\n",
    "    **boostrap**: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.<p>\n",
    "   **max_samples**: If bootstrap is True, the number of samples to draw from X to train each base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Train a [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) on the Bike rentals dataset and evaluate the performance on the same data set partition that we had before. Create a grid search to test different values for the parameters **n_estimators** and **max_samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the validation data is : 228.8176875309073\n"
     ]
    }
   ],
   "source": [
    "# Provide your answer here\n",
    "\n",
    "## import Random Forest package\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## Since Random Forest has many decision tree, we use the same transform as Question 2\n",
    "\n",
    "### We do not use StandardScaler() for the numerical features and keep it as original values\n",
    "Question2_transform = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
    "],remainder='passthrough')\n",
    "\n",
    "\n",
    "## fit transform in the train set\n",
    "bs_train2_set_attributes_Question3 = Question2_transform.fit_transform(bs_train2_set_attributes)\n",
    "## transform in the validation set\n",
    "bs_val_set_attributes_Question3 = Question2_transform.transform(bs_val_set_attributes)\n",
    "\n",
    "## Concatenating the attributes and the labels\n",
    "whole_train_set_attributes_Question3 = np.vstack((bs_train2_set_attributes_Question3 , bs_val_set_attributes_Question3))\n",
    "whole_train_set_labels_Question3 = np.hstack((bs_train2_set_labels, bs_val_set_labels))\n",
    "\n",
    "## Applying the Random Forest for regression and exploring different maximum depth options\n",
    "# For simplicity, we set those values for the parameters n_estimators and max_samples.\n",
    "n_estimators_Question3 = [20, 50, 100, 200]\n",
    "max_samples_Question3 = [500, 1000, 2000, 3000]\n",
    "param_grid_Question3 = dict(n_estimators = n_estimators_Question3, max_samples = max_samples_Question3)\n",
    "grid_regression_Question3 = GridSearchCV(RandomForestRegressor(), param_grid=param_grid_Question3, cv=ps, scoring='neg_mean_squared_error')\n",
    "grid_regression_Question3.fit(whole_train_set_attributes_Question3, whole_train_set_labels_Question3)\n",
    "\n",
    "## Training a Random Forest using the best value for the  n_estimators and max_samples\n",
    "regr_Question3 = RandomForestRegressor(n_estimators=grid_regression_Question3.best_params_[\"n_estimators\"],max_samples=grid_regression_Question3.best_params_[\"max_samples\"])\n",
    "regr_Question3.fit(bs_train2_set_attributes_Question3, bs_train2_set_labels)\n",
    "bs_val_set_predictions_Question3 = regr_Question3.predict(bs_val_set_attributes_Question3)\n",
    "\n",
    "## Computing the RMSE for the validation dataset\n",
    "error_mod_Question3 = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions_Question3))\n",
    "print('The RMSE on the validation data is :',error_mod_Question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "In [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) or [Gradient-boosted trees](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) (GBT), each tree in the ensemble is trained sequentially: the first tree is trained as usual using the training data, the second tree is trained on the residuals between the predictions of the first tree and the labels of the training data, the third tree is trained on the residuals of the predictions of the second tree, etc. The predictions of the ensemble will be the sum of the predictions of each individual tree. The type of residuals are related to the loss function that wants to be minimised.   \n",
    "\n",
    "Scikit-learn uses the classes [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html?highlight=gradient%20boosting#sklearn.ensemble.GradientBoostingRegressor) for the implementation of Gradient-Boosted trees for regression and [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradient%20boosting#sklearn.ensemble.GradientBoostingClassifier) for the implementation of Gradient-Boosted trees for binary classification. \n",
    "\n",
    "Besides the parameters that can be specified for Decision Trees, both classes share some of the additional following parameters\n",
    "\n",
    "> **n_estimators** the number of boosting stages to perform.<p>\n",
    "  **subsample** the fraction of samples to be used for fitting the individual base learners.<p>\n",
    "  **max_features** The number of features to consider when looking for the best split:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Repeat Question 3 but using a [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html?highlight=gradient%20boosting#sklearn.ensemble.GradientBoostingRegressor) using parameters **n_estimators** and **max_features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the validation data is : 240.27533328697913\n"
     ]
    }
   ],
   "source": [
    "# Provide your answer here\n",
    "\n",
    "## I only use a grid search to test different values for the parameters n_estimators \n",
    "## since it does not have max_samples.\n",
    "\n",
    "## import GradientBoostingRegressor package\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "## Since Gradient Boosting Regressor has many decision tree, we use the same transform as Question 2\n",
    "## We do not use StandardScaler() for the numerical features and keep it as original values\n",
    "Question2_transform = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
    "],remainder='passthrough')\n",
    "\n",
    "## fit transform in the train set\n",
    "bs_train2_set_attributes_Question4 = Question2_transform.fit_transform(bs_train2_set_attributes)\n",
    "## transform in the validation set\n",
    "bs_val_set_attributes_Question4 = Question2_transform.transform(bs_val_set_attributes)\n",
    "\n",
    "## Concatenating the attributes and the labels\n",
    "whole_train_set_attributes_Question4 = np.vstack((bs_train2_set_attributes_Question4 , bs_val_set_attributes_Question4))\n",
    "whole_train_set_labels_Question4 = np.hstack((bs_train2_set_labels, bs_val_set_labels))\n",
    "\n",
    "## Applying the Gradient Boosting for regression and exploring different maximum depth options\n",
    "# For simplicity, we set those values for the parameters n_estimators.\n",
    "n_estimators_Question4 = [20, 50, 100, 200]\n",
    "param_grid_Question4 = dict(n_estimators = n_estimators_Question4)\n",
    "grid_regression_Question4 = GridSearchCV(GradientBoostingRegressor(), param_grid=param_grid_Question4, cv=ps, scoring='neg_mean_squared_error')\n",
    "grid_regression_Question4.fit(whole_train_set_attributes_Question4, whole_train_set_labels_Question4)\n",
    "\n",
    "## Training a Gradient Boosting using the best value for the n_estimators\n",
    "regr_Question4 = GradientBoostingRegressor(n_estimators=grid_regression_Question4.best_params_[\"n_estimators\"])\n",
    "regr_Question4.fit(bs_train2_set_attributes_Question4, bs_train2_set_labels)\n",
    "bs_val_set_predictions_Question4 = regr_Question4.predict(bs_val_set_attributes_Question4)\n",
    "\n",
    "## Computing the RMSE for the validation dataset\n",
    "error_mod_Question4 = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions_Question4))\n",
    "print('The RMSE on the validation data is :',error_mod_Question4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
